import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
from model_interface.model_factory import ModelFactory
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    f1_score,
    precision_score,
    recall_score,
)
from tqdm import tqdm

from document_classes import document_classes

# –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–∞ –∫–ª–∞—Å—Å–∞.
CLASS_NAMES_TO_KEYS = {v: k for k, v in document_classes.items()}


def load_config(config_path: str = "config_classification.json") -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        config_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON.

    Returns:
        Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Raises:
        FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏.
    """
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    with config_file.open("r") as f:
        return json.load(f)


def initialize_model(config: Dict[str, Any]) -> Any:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Args:
        config (Dict[str, Any]): –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ 'model_family', 'model_name',
            'cache_dir', 'device_map', 'package', 'module', 'model_class'.

    Returns:
        Any: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏.
    """
    model_family = config["model_family"]
    cache_dir = Path(config["cache_dir"])
    cache_dir.mkdir(exist_ok=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∫–ª–∞—Å—Å—É –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    package = config["package"]
    module = config["module"]
    model_class = config["model_class"]
    model_class_path = f"{package}.{module}:{model_class}"

    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–±—Ä–∏–∫–µ
    ModelFactory.register_model(model_family, model_class_path)

    model_params = {
        "model_name": config["model_name"],
        "system_prompt": config.get("system_prompt", ""),
        "cache_dir": config["cache_dir"],
        "device_map": config["device_map"],
    }
    print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: {config['model_name']}")
    return ModelFactory.get_model(model_family, model_params)


def get_image_paths(
    dataset_path: Path,
    class_names: List[str],
    subset_name: str,
    sample_size: Optional[int] = None,
) -> List[Path]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.

    –§—É–Ω–∫—Ü–∏—è –æ–±—Ö–æ–¥–∏—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –∏ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤, —Å–æ–±–∏—Ä–∞—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º.
    –û–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞—Ö–æ–¥—è—Ç—Å—è –∫–∞–∫
    –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø–∞–ø–∫–µ —Å–∞–±—Å–µ—Ç–∞, —Ç–∞–∫ –∏ –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–¥–ø–∞–ø–∫–∞—Ö.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        class_names (List[str]): –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        subset_name (str): –ò–º—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'clean', 'blur').
        sample_size (Optional[int]): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ –∏–∑ –∫–∞–∂–¥–æ–π
            –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–ª–∞—Å—Å–∞. –ï—Å–ª–∏ None, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Å–µ —Ñ–∞–π–ª—ã.

    Returns:
        List[Path]: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Path, –≤–µ–¥—É—â–∏—Ö –∫ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.
    """
    selected_files = []
    print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–±—Å–µ—Ç–∞: {subset_name}")
    for class_name in class_names:
        class_dir = dataset_path / class_name / "images" / subset_name
        if not class_dir.exists():
            continue

        paths = list(class_dir.iterdir())
        if sample_size is not None:
            paths = paths[:sample_size]

        for path in paths:
            if path.is_file():
                selected_files.append(path)
            else:
                selected_files.extend(p for p in path.iterdir() if p.is_file())

    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(selected_files)}")
    return selected_files


def get_prediction(model: Any, image_path: Path, prompt: str) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

    Args:
        model (Any): –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        image_path (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        prompt (str): –ü—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø–æ–¥–∞–Ω –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º.

    Returns:
        str: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª—é—á –∫–ª–∞—Å—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'invoice') –∏–ª–∏ 'None'
             –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.
    """
    try:
        # –ü–µ—Ä–µ–¥–∞–µ–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –Ω–∞–ø—Ä—è–º—É—é –≤ –º–æ–¥–µ–ª—å
        result = model.predict_on_image(image=str(image_path), question=prompt)
        prediction = result.strip().strip('"')

        if prediction.isdigit():
            class_index = int(prediction)
            if 0 <= class_index < len(document_classes):
                pred_class_name = list(document_classes.values())[class_index]
                return CLASS_NAMES_TO_KEYS.get(pred_class_name, "None")
        return "None"

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {image_path.name}: {e}")
        return "None"


def calculate_and_save_metrics(
    y_true: List[str], y_pred: List[str], subset_name: str, run_id: str
) -> Dict[str, float]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.

    –°–æ–∑–¥–∞–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ CSV-—Ñ–∞–π–ª —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞,
    –∞ —Ç–∞–∫–∂–µ CSV-—Ñ–∞–π–ª —Å –æ–±—â–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ (accuracy, f1, precision, recall).

    Args:
        y_true (List[str]): –°–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤.
        y_pred (List[str]): –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤.
        subset_name (str): –ò–º—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞.
        run_id (str): –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—É—Å–∫–∞ –¥–ª—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤.

    Returns:
        Dict[str, float]: –°–ª–æ–≤–∞—Ä—å —Å –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
                          –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –Ω–µ—Ç.
    """
    if not y_true:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫.")
        return {}

    all_classes = list(document_classes.keys())
    if "None" in set(y_pred):
        all_classes.append("None")

    print(f"\nüìä –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —Å–∞–±—Å–µ—Ç–∞ {subset_name}:")
    report = classification_report(
        y_true, y_pred, labels=all_classes, output_dict=True, zero_division=0
    )
    report_df = pd.DataFrame(report).transpose()
    report_df.to_csv(f"{run_id}_{subset_name}_per_class_metrics.csv")
    print(report_df)

    metrics = {
        "accuracy": accuracy_score(y_true, y_pred),
        "f1": f1_score(y_true, y_pred, average="weighted", zero_division=0),
        "precision": precision_score(
            y_true, y_pred, average="weighted", zero_division=0
        ),
        "recall": recall_score(y_true, y_pred, average="weighted", zero_division=0),
    }

    results_df = pd.DataFrame([metrics])
    results_df.to_csv(f"{run_id}_{subset_name}_classification_results.csv", index=False)

    return metrics


def run_evaluation(config: Dict[str, Any]) -> None:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.

    –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å: –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    –º–æ–¥–µ–ª–∏ –¥–æ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞–º, —Å–±–æ—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ —Ä–∞—Å—á–µ—Ç–∞
    –∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫.

    Args:
        config (Dict[str, Any]): –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –¥–ª—è –∑–∞–ø—É—Å–∫–∞.
    """
    dataset_path = Path(config["dataset_path"])
    prompt_path = Path(config["prompt_path"])
    sample_size = config.get("sample_size")

    model = initialize_model(config)

    prompt_template = prompt_path.read_text(encoding="utf-8")
    classes_str = ", ".join(
        f"{idx}: {name}" for idx, name in enumerate(document_classes.values())
    )
    prompt = prompt_template.format(classes=classes_str)

    run_id = Path(config["model_name"]).stem
    all_metrics = []

    for subset in config["subsets"]:
        image_paths = get_image_paths(
            dataset_path, list(document_classes.keys()), subset, sample_size
        )

        if not image_paths:
            continue

        y_true, y_pred = [], []
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `path.parts` –¥–µ–ª–∞–µ—Ç –∫–æ–¥ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º –æ—Ç –û–°.
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—É—Ç–∏: ./dataset/{class_name}/images/{subset_name}/...
        # –ü–æ—ç—Ç–æ–º—É `class_name` —ç—Ç–æ 4-–π —ç–ª–µ–º–µ–Ω—Ç —Å –∫–æ–Ω—Ü–∞ (–∏–Ω–¥–µ–∫—Å -4).
        for path in tqdm(image_paths, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {subset}"):
            y_true.append(path.parts[-4])
            y_pred.append(get_prediction(model, path, prompt))

        subset_metrics = calculate_and_save_metrics(y_true, y_pred, subset, run_id)
        if subset_metrics:
            all_metrics.append(subset_metrics)

    if all_metrics:
        final_df = pd.DataFrame(all_metrics)
        avg_metrics = final_df.mean()
        print("\nüìä –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–∞–±—Å–µ—Ç–∞–º:")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {avg_metrics['accuracy']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π F1-score: {avg_metrics['f1']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (Precision): {avg_metrics['precision']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π –æ—Ç–∑—ã–≤ (Recall): {avg_metrics['recall']:.4f}")

        final_df.to_csv(f"{run_id}_final_classification_results.csv", index=False)


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏.
    """
    try:
        config = load_config()
        run_evaluation(config)
    except (FileNotFoundError, KeyError) as e:
        print(f"–û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
