import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
from tqdm import tqdm

from bench_utils.metrics import calculate_ordering_metrics
from model_interface.model_factory import ModelFactory, load_prompt
from bench_utils.utils import (
    get_document_type_from_config,
    get_run_id,
    load_config,
)


def get_image_paths_for_document(
    dataset_path: Path, document_id: str, subset_name: str
) -> List[Path]:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        document_id (str): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        subset_name (str): –ò–º—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'clean', 'blur').

    Returns:
        List[Path]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ø–æ—Ä—è–¥–∫–µ –Ω–æ–º–µ—Ä–æ–≤.
    """
    document_dir = dataset_path / "images" / subset_name / document_id
    if not document_dir.exists():
        return []

    image_files = []
    for i in range(10):
        image_path = document_dir / f"{i}.jpg"
        if image_path.exists():
            image_files.append(image_path)
        else:
            break

    return image_files


def get_document_ids(
    dataset_path: Path, subset_name: str, sample_size: Optional[int] = None
) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        subset_name (str): –ò–º—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞.
        sample_size (Optional[int]): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏.
                                   –ï—Å–ª–∏ None, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    subset_dir = dataset_path / "images" / subset_name
    if not subset_dir.exists():
        return []

    document_ids = [d.name for d in subset_dir.iterdir() if d.is_dir()]

    if sample_size is not None:
        document_ids = document_ids[:sample_size]

    return document_ids


def load_ground_truth_dynamic(
    dataset_path: Path, document_id: str, document_type_key: str
) -> List[int]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ JSON —Ñ–∞–π–ª–∞ –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        document_id (str): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        document_type_key (str): –ö–ª—é—á —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ JSON —Ñ–∞–π–ª–µ.

    Returns:
        List[int]: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü (–∏–Ω–¥–µ–∫—Å—ã –æ—Ç 1).
    """
    json_file = dataset_path / "jsons" / f"{document_id}.json"
    if not json_file.exists():
        return []

    with json_file.open("r", encoding="utf-8") as f:
        data = json.load(f)

    fields = data.get("fields", {})
    if document_type_key not in fields:
        print(f"–ö–ª—é—á '{document_type_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ {json_file}")
        print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(fields.keys())}")
        return []

    true_order = fields[document_type_key]
    return [i + 1 for i in true_order]


def extract_json_from_model_output(model_output: str) -> Optional[Dict[str, Any]]:
    if not isinstance(model_output, str):
        print(f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–∞, –ø–æ–ª—É—á–µ–Ω: {type(model_output)}")
        return None

    cleaned_output = model_output.strip()

    json_match = re.search(r"```(?:json)?\s*\n(.*?)\n```", cleaned_output, re.DOTALL)
    if json_match:
        json_content = json_match.group(1).strip()
    else:
        json_match = re.search(r"\{.*\}", cleaned_output, re.DOTALL)
        if json_match:
            json_content = json_match.group(0).strip()
        else:
            json_content = cleaned_output

    try:
        return json.loads(json_content)
    except json.JSONDecodeError as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {repr(json_content[:200])}...")
        return None


def extract_ordered_pages_from_json(parsed_json: Dict[str, Any]) -> List[int]:
    if not isinstance(parsed_json, dict):
        print(f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å, –ø–æ–ª—É—á–µ–Ω: {type(parsed_json)}")
        return []

    if "ordered_pages" in parsed_json:
        pages = parsed_json["ordered_pages"]
        if isinstance(pages, list) and all(isinstance(p, int) for p in pages):
            return pages
        else:
            print(
                f"–ó–Ω–∞—á–µ–Ω–∏–µ 'ordered_pages' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –ø–æ–ª—É—á–µ–Ω–æ: {pages}"
            )
    else:
        print(
            f"–ö–ª—é—á 'ordered_pages' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ JSON. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(parsed_json.keys())}"
        )

    return []


def parse_model_output_fallback(model_output: str) -> List[int]:
    array_match = re.search(r"\[[\d\s,]+\]", model_output)
    if array_match:
        try:
            return json.loads(array_match.group(0))
        except json.JSONDecodeError:
            pass

    numbers = re.findall(r"\b[1-9]\b", model_output)
    if numbers:
        try:
            return [int(n) for n in numbers[:4]]
        except ValueError:
            pass

    return []


def process_model_response(model_response: str) -> List[int]:
    if not isinstance(model_response, str):
        print(f"–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø: {type(model_response)}")
        return []

    parsed_json = extract_json_from_model_output(model_response)
    if parsed_json:
        ordered_pages = extract_ordered_pages_from_json(parsed_json)
        if ordered_pages:
            return ordered_pages

    print("–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–±...")
    fallback_result = parse_model_output_fallback(model_response)
    if fallback_result:
        print(f"–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {fallback_result}")
        return fallback_result

    print("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏")
    print(f"–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: {repr(model_response[:300])}...")
    return []


def get_prediction(model: Any, image_paths: List[Path], prompt: str) -> List[int]:
    try:
        image_paths_str = [str(path) for path in image_paths]
        model_response = model.predict_on_images(
            images=image_paths_str, question=prompt
        )
        return process_model_response(model_response)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        return []


def save_prediction(output_dir: Path, document_id: str, prediction: List[int]) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"{document_id}.json"

    result = {"ordered_pages": prediction}
    with output_file.open("w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def calculate_and_save_metrics(
    all_metrics: Dict[str, List[float]], subset_name: str, run_id: str
) -> Dict[str, float]:
    if not all_metrics or not any(all_metrics.values()):
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫.")
        return {}

    mean_metrics = {
        key: round(sum(values) / len(values), 4) if values else 0.0
        for key, values in all_metrics.items()
    }

    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∞–±—Å–µ—Ç–∞ {subset_name}:")
    for key, value in mean_metrics.items():
        print(f"  {key}: {value:.4f}")

    results_df = pd.DataFrame([mean_metrics])
    results_df.to_csv(f"{run_id}_{subset_name}_page_sorting_results.csv", index=False)

    return mean_metrics


def run_evaluation(config: Dict[str, Any]) -> None:
    task_config = config["task"]
    model_config = config["model"]

    dataset_path = Path(task_config["dataset_path"])
    prompt_path = Path(task_config["prompt_path"])
    sample_size = task_config.get("sample_size")
    output_base_dir = Path(task_config["output_dir"])

    model = ModelFactory.initialize_model(model_config)

    prompt = load_prompt(prompt_path)
    run_id = get_run_id(model_config["model_name"])

    document_type_name = get_document_type_from_config(config, dataset_path)
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–∏–ø–∞: {document_type_name}")

    document_type_key = None
    for key, name in config.get("document_classes", {}).items():
        if key in dataset_path.name:
            document_type_key = key
            break

    if not document_type_key:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø—É—Ç–∏ {dataset_path}")
        return

    all_subset_metrics = []

    for subset in task_config["subsets"]:
        print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–±—Å–µ—Ç–∞: {subset}")

        document_ids = get_document_ids(dataset_path, subset, sample_size)
        if not document_ids:
            print(f"–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∞–±—Å–µ—Ç–µ {subset}")
            continue

        print(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(document_ids)}")

        output_dir = output_base_dir / dataset_path.name / subset

        all_metrics = {
            "kendall_tau": [],
            "accuracy": [],
            "spearman_rho": [],
        }

        for doc_id in tqdm(document_ids, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {subset}"):
            image_paths = get_image_paths_for_document(dataset_path, doc_id, subset)
            if len(image_paths) != 4:
                print(
                    f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}: –æ–∂–∏–¥–∞–µ—Ç—Å—è 4 —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –Ω–∞–π–¥–µ–Ω–æ {len(image_paths)}"
                )
                continue

            true_order = load_ground_truth_dynamic(
                dataset_path, doc_id, document_type_key
            )
            if not true_order:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}")
                continue

            predicted_order = get_prediction(model, image_paths, prompt)
            if not predicted_order:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}")
                continue

            save_prediction(output_dir, doc_id, predicted_order)

            metrics = calculate_ordering_metrics(true_order, predicted_order)
            for key, value in metrics.items():
                all_metrics[key].append(value)

            print(f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}: {metrics}")

        subset_metrics = calculate_and_save_metrics(all_metrics, subset, run_id)
        if subset_metrics:
            all_subset_metrics.append(subset_metrics)

    if all_subset_metrics:
        final_df = pd.DataFrame(all_subset_metrics)
        overall_metrics = final_df.mean()

        print(f"\nüìä –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–∞–±—Å–µ—Ç–∞–º –¥–ª—è {document_type_name}:")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {overall_metrics['accuracy']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π Kendall Tau: {overall_metrics['kendall_tau']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π Spearman Rho: {overall_metrics['spearman_rho']:.4f}")

        final_df.to_csv(f"{run_id}_final_page_sorting_results.csv", index=False)


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü.

    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏.
    """
    try:
        config = load_config("config_page_sorting.json")
        run_evaluation(config)
    except (FileNotFoundError, KeyError) as e:
        print(f"–û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
