import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
from model_interface.model_factory import ModelFactory
from scipy.stats import kendalltau, spearmanr
from tqdm import tqdm


def load_config(config_path: str = "config_page_sorting.json") -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        config_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON.

    Returns:
        Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Raises:
        FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏.
    """
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    with config_file.open("r") as f:
        return json.load(f)


def initialize_model(config: Dict[str, Any]) -> Any:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Args:
        config (Dict[str, Any]): –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å–µ–∫—Ü–∏–∏
            'model' —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏.

    Returns:
        Any: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏.
    """
    model_config = config["model"]

    model_family = model_config["model_family"]
    cache_dir = Path(model_config["cache_dir"])
    cache_dir.mkdir(exist_ok=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∫–ª–∞—Å—Å—É –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    package = model_config["package"]
    module = model_config["module"]
    model_class = model_config["model_class"]
    model_class_path = f"{package}.{module}:{model_class}"

    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–±—Ä–∏–∫–µ
    ModelFactory.register_model(model_family, model_class_path)

    model_params = {
        "model_name": model_config["model_name"],
        "system_prompt": model_config.get("system_prompt", ""),
        "cache_dir": model_config["cache_dir"],
        "device_map": model_config["device_map"],
    }
    print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: {model_config['model_name']}")
    return ModelFactory.get_model(model_family, model_params)


def get_image_paths_for_document(
    dataset_path: Path, document_id: str, subset_name: str
) -> List[Path]:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        document_id (str): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        subset_name (str): –ò–º—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'clean', 'blur').

    Returns:
        List[Path]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ø–æ—Ä—è–¥–∫–µ –Ω–æ–º–µ—Ä–æ–≤.
    """
    document_dir = dataset_path / "images" / subset_name / document_id
    if not document_dir.exists():
        return []

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –Ω–æ–º–µ—Ä–∞–º —Å—Ç—Ä–∞–Ω–∏—Ü (0.jpg, 1.jpg, 2.jpg, 3.jpg)
    image_files = []
    for i in range(10):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
        image_path = document_dir / f"{i}.jpg"
        if image_path.exists():
            image_files.append(image_path)
        else:
            break

    return image_files


def get_document_ids(
    dataset_path: Path, subset_name: str, sample_size: Optional[int] = None
) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        subset_name (str): –ò–º—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞.
        sample_size (Optional[int]): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏.
                                   –ï—Å–ª–∏ None, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    subset_dir = dataset_path / "images" / subset_name
    if not subset_dir.exists():
        return []

    document_ids = [d.name for d in subset_dir.iterdir() if d.is_dir()]

    if sample_size is not None:
        document_ids = document_ids[:sample_size]

    return document_ids


def load_ground_truth(dataset_path: Path, document_id: str) -> List[int]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        document_id (str): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Returns:
        List[int]: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü (–∏–Ω–¥–µ–∫—Å—ã –æ—Ç 1).
    """
    json_file = dataset_path / "jsons" / f"{document_id}.json"
    if not json_file.exists():
        return []

    with json_file.open("r", encoding="utf-8") as f:
        data = json.load(f)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ 0-based –≤ 1-based
    true_order = data["fields"]["interest_free_loan_agreement"]
    return [i + 1 for i in true_order]


def get_document_type_from_config(config: Dict[str, Any], dataset_path: Path) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.

    Args:
        config (Dict[str, Any]): –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        dataset_path (Path): –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.

    Returns:
        str: –†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    document_classes = config.get("document_classes", {})

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –ø—É—Ç–∏
    dataset_name = dataset_path.name

    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for doc_type, doc_name in document_classes.items():
        if doc_type in dataset_name:
            return doc_name

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
    return dataset_name.replace("_", " ").title()


def load_ground_truth_dynamic(
    dataset_path: Path, document_id: str, document_type_key: str
) -> List[int]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ JSON —Ñ–∞–π–ª–∞ –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        dataset_path (Path): –ö–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É.
        document_id (str): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        document_type_key (str): –ö–ª—é—á —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ JSON —Ñ–∞–π–ª–µ.

    Returns:
        List[int]: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü (–∏–Ω–¥–µ–∫—Å—ã –æ—Ç 1).
    """
    json_file = dataset_path / "jsons" / f"{document_id}.json"
    if not json_file.exists():
        return []

    with json_file.open("r", encoding="utf-8") as f:
        data = json.load(f)

    # –ò—â–µ–º –∫–ª—é—á —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ fields
    fields = data.get("fields", {})
    if document_type_key not in fields:
        print(f"–ö–ª—é—á '{document_type_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ {json_file}")
        print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(fields.keys())}")
        return []

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ 0-based –≤ 1-based
    true_order = fields[document_type_key]
    return [i + 1 for i in true_order]


def extract_json_from_model_output(model_output: str) -> Optional[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç JSON –∏–∑ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã.

    –§—É–Ω–∫—Ü–∏—è —É–º–µ–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å:
    - JSON –æ–±–µ—Ä–Ω—É—Ç—ã–π –≤ markdown –±–ª–æ–∫–∏ (```json ... ```)
    - –ß–∏—Å—Ç—ã–π JSON –±–µ–∑ –æ–±–µ—Ä—Ç–æ–∫
    - JSON —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –≤–æ–∫—Ä—É–≥

    Args:
        model_output (str): –°—ã—Ä–æ–π –≤—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏.

    Returns:
        Optional[Dict[str, Any]]: –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    if not isinstance(model_output, str):
        print(f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–∞, –ø–æ–ª—É—á–µ–Ω: {type(model_output)}")
        return None

    cleaned_output = model_output.strip()

    # –ü–æ–ø—ã—Ç–∫–∞ 1: –ü–æ–∏—Å–∫ JSON –≤ markdown –±–ª–æ–∫–µ
    json_match = re.search(r"```(?:json)?\s*\n(.*?)\n```", cleaned_output, re.DOTALL)
    if json_match:
        json_content = json_match.group(1).strip()
    else:
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–æ–∏—Å–∫ JSON –±–ª–æ–∫–∞ –ø–æ —Ñ–∏–≥—É—Ä–Ω—ã–º —Å–∫–æ–±–∫–∞–º
        json_match = re.search(r"\{.*\}", cleaned_output, re.DOTALL)
        if json_match:
            json_content = json_match.group(0).strip()
        else:
            # –ü–æ–ø—ã—Ç–∫–∞ 3: –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –∫–∞–∫ –µ—Å—Ç—å
            json_content = cleaned_output

    # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
    try:
        return json.loads(json_content)
    except json.JSONDecodeError as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {repr(json_content[:200])}...")
        return None


def extract_ordered_pages_from_json(parsed_json: Dict[str, Any]) -> List[int]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ JSON.

    Args:
        parsed_json (Dict[str, Any]): –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏.

    Returns:
        List[int]: –°–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
    """
    if not isinstance(parsed_json, dict):
        print(f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å, –ø–æ–ª—É—á–µ–Ω: {type(parsed_json)}")
        return []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ ordered_pages
    if "ordered_pages" in parsed_json:
        pages = parsed_json["ordered_pages"]
        if isinstance(pages, list) and all(isinstance(p, int) for p in pages):
            return pages
        else:
            print(
                f"–ó–Ω–∞—á–µ–Ω–∏–µ 'ordered_pages' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –ø–æ–ª—É—á–µ–Ω–æ: {pages}"
            )
    else:
        print(
            f"–ö–ª—é—á 'ordered_pages' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ JSON. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(parsed_json.keys())}"
        )

    return []


def parse_model_output_fallback(model_output: str) -> List[int]:
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–∏—Å–µ–ª –∏–∑ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏.

    –ò—â–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ –º–∞—Å—Å–∏–≤—ã —á–∏—Å–µ–ª –≤–∏–¥–∞ [1, 2, 3, 4] –∏–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–∏—Å–µ–ª.

    Args:
        model_output (str): –°—ã—Ä–æ–π –≤—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏.

    Returns:
        List[int]: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
    """
    # –ü–æ–∏—Å–∫ –º–∞—Å—Å–∏–≤–∞ —á–∏—Å–µ–ª –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö
    array_match = re.search(r"\[[\d\s,]+\]", model_output)
    if array_match:
        try:
            return json.loads(array_match.group(0))
        except json.JSONDecodeError:
            pass

    # –ü–æ–∏—Å–∫ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª
    numbers = re.findall(r"\b[1-9]\b", model_output)
    if numbers:
        try:
            return [int(n) for n in numbers[:4]]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 4 —á–∏—Å–ª–∞
        except ValueError:
            pass

    return []


def process_model_response(model_response: str) -> List[int]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü.

    Args:
        model_response (str): –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏.

    Returns:
        List[int]: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    if not isinstance(model_response, str):
        print(f"–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø: {type(model_response)}")
        return []

    # –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–æ—Å–æ–±: –ø–∞—Ä—Å–∏–Ω–≥ JSON
    parsed_json = extract_json_from_model_output(model_response)
    if parsed_json:
        ordered_pages = extract_ordered_pages_from_json(parsed_json)
        if ordered_pages:
            return ordered_pages

    # –†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: –ø–æ–∏—Å–∫ —á–∏—Å–µ–ª –≤ —Ç–µ–∫—Å—Ç–µ
    print("–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–±...")
    fallback_result = parse_model_output_fallback(model_response)
    if fallback_result:
        print(f"–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {fallback_result}")
        return fallback_result

    print("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏")
    print(f"–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: {repr(model_response[:300])}...")
    return []


def get_prediction(model: Any, image_paths: List[Path], prompt: str) -> List[int]:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        model (Any): –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏.
        image_paths (List[Path]): –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–∞–Ω–∏—Ü.
        prompt (str): –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏.

    Returns:
        List[int]: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—É—Ç–∏ –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å
        image_paths_str = [str(path) for path in image_paths]

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        model_response = model.predict_on_images(
            images=image_paths_str, question=prompt
        )

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        return process_model_response(model_response)

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        return []


def evaluate_ordering(
    true_order: List[int], predicted_order: List[int]
) -> Dict[str, float]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü.

    Args:
        true_order (List[int]): –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü.
        predicted_order (List[int]): –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü.

    Returns:
        Dict[str, float]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (kendall_tau, accuracy, spearman_rho).
    """
    if not true_order or not predicted_order or len(true_order) != len(predicted_order):
        return {"kendall_tau": 0.0, "accuracy": 0.0, "spearman_rho": 0.0}

    if set(true_order) != set(predicted_order):
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–∞–±–æ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç")
        print(f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π: {true_order}")
        print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: {predicted_order}")
        return {"kendall_tau": 0.0, "accuracy": 0.0, "spearman_rho": 0.0}

    # –°–ª–æ–≤–∞—Ä—å: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ‚Üí –µ—ë –ø–æ–∑–∏—Ü–∏—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    true_positions = {page: i for i, page in enumerate(true_order)}

    # –°–æ–ø–æ—Å—Ç–∞–≤–∏–º –ø–æ–∑–∏—Ü–∏–∏ predicted_order —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏
    true_ranks = [true_positions[page] for page in predicted_order]
    pred_ranks = list(range(len(predicted_order)))

    # –ú–µ—Ç—Ä–∏–∫–∏
    kendall, _ = kendalltau(pred_ranks, true_ranks)
    accuracy = sum(t == p for t, p in zip(true_order, predicted_order)) / len(
        true_order
    )
    rho, _ = spearmanr(true_order, predicted_order)

    return {
        "kendall_tau": round(kendall, 4),
        "accuracy": round(accuracy, 4),
        "spearman_rho": round(rho, 4),
    }


def save_prediction(output_dir: Path, document_id: str, prediction: List[int]) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ JSON —Ñ–∞–π–ª.

    Args:
        output_dir (Path): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        document_id (str): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        prediction (List[int]): –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"{document_id}.json"

    result = {"ordered_pages": prediction}
    with output_file.open("w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def calculate_and_save_metrics(
    all_metrics: Dict[str, List[float]], subset_name: str, run_id: str
) -> Dict[str, float]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

    Args:
        all_metrics (Dict[str, List[float]]): –°–ª–æ–≤–∞—Ä—å —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ –º–µ—Ç—Ä–∏–∫.
        subset_name (str): –ò–º—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞.
        run_id (str): –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—É—Å–∫–∞.

    Returns:
        Dict[str, float]: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ä–µ–¥–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.
    """
    if not all_metrics or not any(all_metrics.values()):
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫.")
        return {}

    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    mean_metrics = {
        key: round(sum(values) / len(values), 4) if values else 0.0
        for key, values in all_metrics.items()
    }

    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∞–±—Å–µ—Ç–∞ {subset_name}:")
    for key, value in mean_metrics.items():
        print(f"  {key}: {value:.4f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_df = pd.DataFrame([mean_metrics])
    results_df.to_csv(f"{run_id}_{subset_name}_page_sorting_results.csv", index=False)

    return mean_metrics


def run_evaluation(config: Dict[str, Any]) -> None:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á–∏ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü.

    –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å: –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    –º–æ–¥–µ–ª–∏ –¥–æ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞–º, —Å–±–æ—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ —Ä–∞—Å—á–µ—Ç–∞
    –∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫.

    Args:
        config (Dict[str, Any]): –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –¥–ª—è –∑–∞–ø—É—Å–∫–∞,
                                —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å–µ–∫—Ü–∏–∏ 'task' –∏ 'model'.
    """
    task_config = config["task"]
    model_config = config["model"]

    dataset_path = Path(task_config["dataset_path"])
    prompt_path = Path(task_config["prompt_path"])
    sample_size = task_config.get("sample_size")
    output_base_dir = Path(task_config["output_dir"])  # –¢–µ–ø–µ—Ä—å –±–µ—Ä–µ–º –∏–∑ task

    model = initialize_model(config)

    prompt = prompt_path.read_text(encoding="utf-8")
    run_id = Path(model_config["model_name"]).stem

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
    document_type_name = get_document_type_from_config(config, dataset_path)
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–∏–ø–∞: {document_type_name}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ JSON —Ñ–∞–π–ª–∞—Ö
    document_type_key = None
    for key, name in config.get("document_classes", {}).items():
        if key in dataset_path.name:
            document_type_key = key
            break

    if not document_type_key:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø—É—Ç–∏ {dataset_path}")
        return

    all_subset_metrics = []

    for subset in task_config["subsets"]:
        print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–±—Å–µ—Ç–∞: {subset}")

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ
        document_ids = get_document_ids(dataset_path, subset, sample_size)
        if not document_ids:
            print(f"–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∞–±—Å–µ—Ç–µ {subset}")
            continue

        print(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(document_ids)}")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_dir = output_base_dir / dataset_path.name / subset

        all_metrics = {
            "kendall_tau": [],
            "accuracy": [],
            "spearman_rho": [],
        }

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for doc_id in tqdm(document_ids, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {subset}"):
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–∞–Ω–∏—Ü
            image_paths = get_image_paths_for_document(dataset_path, doc_id, subset)
            if len(image_paths) != 4:
                print(
                    f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}: –æ–∂–∏–¥–∞–µ—Ç—Å—è 4 —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –Ω–∞–π–¥–µ–Ω–æ {len(image_paths)}"
                )
                continue

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–ª—é—á–∞
            true_order = load_ground_truth_dynamic(
                dataset_path, doc_id, document_type_key
            )
            if not true_order:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}")
                continue

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            predicted_order = get_prediction(model, image_paths, prompt)
            if not predicted_order:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}")
                continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            save_prediction(output_dir, doc_id, predicted_order)

            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = evaluate_ordering(true_order, predicted_order)
            for key, value in metrics.items():
                all_metrics[key].append(value)

            print(f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}: {metrics}")

        # –í—ã—á–∏—Å–ª—è–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞
        subset_metrics = calculate_and_save_metrics(all_metrics, subset, run_id)
        if subset_metrics:
            all_subset_metrics.append(subset_metrics)

    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞–º
    if all_subset_metrics:
        final_df = pd.DataFrame(all_subset_metrics)
        overall_metrics = final_df.mean()

        print(f"\nüìä –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–∞–±—Å–µ—Ç–∞–º –¥–ª—è {document_type_name}:")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {overall_metrics['accuracy']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π Kendall Tau: {overall_metrics['kendall_tau']:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π Spearman Rho: {overall_metrics['spearman_rho']:.4f}")

        final_df.to_csv(f"{run_id}_final_page_sorting_results.csv", index=False)


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü.

    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ—Ü–µ–Ω–∫–∏.
    """
    try:
        config = load_config()
        run_evaluation(config)
    except (FileNotFoundError, KeyError) as e:
        print(f"–û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
